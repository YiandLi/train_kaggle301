{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "/root/miniconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from multiprocesspandas import applyparallel\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import regex as re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "topic_df = pd.read_csv('topics.csv')\n",
    "content_df = pd.read_csv('content.csv')\n",
    "corr_df = pd.read_csv('correlations.csv')\n",
    "# topic_df = topic_df.rename(columns={'id': 'topic_id'}).merge(corr_df)\n",
    "topic_df_non_source = topic_df[topic_df['category']!='source'].reset_index(drop=True)\n",
    "topic_df_non_source['stratify'] = topic_df_non_source['category'] + \\\n",
    "topic_df_non_source['language'] + topic_df_non_source['description'].apply(lambda x: str(isinstance(x, str))) + \\\n",
    "topic_df_non_source['has_content'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py:885: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedGroupKFold(n_splits=N_SPLITS)\n",
    "folds = list(kf.split(topic_df_non_source, y=topic_df_non_source[\"stratify\"], groups=topic_df_non_source[\"channel\"]))\n",
    "topic_df_non_source['fold'] = -1\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "    topic_df_non_source.loc[val_idx, \"fold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_df =  topic_df.merge(topic_df_non_source[['id', 'fold']], on='id', how='left').reset_index(drop=True)[['id', 'fold']].fillna(-1).rename(columns={'id': 'topic_id'})\n",
    "fold_df['fold'] = fold_df['fold'].astype(int)\n",
    "corr_df['content_ids'] = corr_df['content_ids'].apply(lambda x:x.split())\n",
    "corr_df = corr_df.explode('content_ids').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = topic_df.fillna('')\n",
    "topic_df['topic_full_text'] =  topic_df['title'] + ' [SEP] ' + topic_df['description']\n",
    "topic_df = topic_df[['id','title' ,'topic_full_text', 'language']]\n",
    "df = corr_df.merge(topic_df, left_on='topic_id', right_on='id', how='left')\n",
    "df = df[['topic_id','content_ids','topic_full_text','language', 'title']]\n",
    "df = df.rename(columns={'language':'topic_language', 'title': 'topic_title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df = content_df.fillna('')\n",
    "content_df['content_full_text'] =  content_df['title'] + ' [SEP] ' + content_df['description'] + ' [SEP] ' + content_df['text']\n",
    "content_df = content_df[['id', 'title', 'content_full_text', 'language']]\n",
    "df = df.merge(content_df, left_on='content_ids', right_on='id', how='left')\n",
    "df = df.rename(columns={'language':'content_language', 'title': 'content_title'})\n",
    "df['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(fold_df, on='topic_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "      <th>topic_full_text</th>\n",
       "      <th>topic_language</th>\n",
       "      <th>topic_title</th>\n",
       "      <th>id</th>\n",
       "      <th>content_title</th>\n",
       "      <th>content_full_text</th>\n",
       "      <th>content_language</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_1108dd0c7a5d</td>\n",
       "      <td>Откриването на резисторите [SEP] Изследване на...</td>\n",
       "      <td>bg</td>\n",
       "      <td>Откриването на резисторите</td>\n",
       "      <td>c_1108dd0c7a5d</td>\n",
       "      <td>Молив като резистор</td>\n",
       "      <td>Молив като резистор [SEP] Моливът причинява пр...</td>\n",
       "      <td>bg</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_376c5a8eb028</td>\n",
       "      <td>Откриването на резисторите [SEP] Изследване на...</td>\n",
       "      <td>bg</td>\n",
       "      <td>Откриването на резисторите</td>\n",
       "      <td>c_376c5a8eb028</td>\n",
       "      <td>Да чуем променливото съпротивление</td>\n",
       "      <td>Да чуем променливото съпротивление [SEP] Тук ч...</td>\n",
       "      <td>bg</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_5bc0e1e2cba0</td>\n",
       "      <td>Откриването на резисторите [SEP] Изследване на...</td>\n",
       "      <td>bg</td>\n",
       "      <td>Откриването на резисторите</td>\n",
       "      <td>c_5bc0e1e2cba0</td>\n",
       "      <td>Променлив резистор (реостат) с графит от молив</td>\n",
       "      <td>Променлив резистор (реостат) с графит от молив...</td>\n",
       "      <td>bg</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_76231f9d0b5e</td>\n",
       "      <td>Откриването на резисторите [SEP] Изследване на...</td>\n",
       "      <td>bg</td>\n",
       "      <td>Откриването на резисторите</td>\n",
       "      <td>c_76231f9d0b5e</td>\n",
       "      <td>Последователно свързване на галваничен елемент...</td>\n",
       "      <td>Последователно свързване на галваничен елемент...</td>\n",
       "      <td>bg</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_00068291e9a4</td>\n",
       "      <td>c_639ea2ef9c95</td>\n",
       "      <td>Entradas e saídas de uma função [SEP] Entenda ...</td>\n",
       "      <td>pt</td>\n",
       "      <td>Entradas e saídas de uma função</td>\n",
       "      <td>c_639ea2ef9c95</td>\n",
       "      <td>Dados e resultados de funções: gráficos</td>\n",
       "      <td>Dados e resultados de funções: gráficos [SEP] ...</td>\n",
       "      <td>pt</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_id     content_ids  \\\n",
       "0  t_00004da3a1b2  c_1108dd0c7a5d   \n",
       "1  t_00004da3a1b2  c_376c5a8eb028   \n",
       "2  t_00004da3a1b2  c_5bc0e1e2cba0   \n",
       "3  t_00004da3a1b2  c_76231f9d0b5e   \n",
       "4  t_00068291e9a4  c_639ea2ef9c95   \n",
       "\n",
       "                                     topic_full_text topic_language  \\\n",
       "0  Откриването на резисторите [SEP] Изследване на...             bg   \n",
       "1  Откриването на резисторите [SEP] Изследване на...             bg   \n",
       "2  Откриването на резисторите [SEP] Изследване на...             bg   \n",
       "3  Откриването на резисторите [SEP] Изследване на...             bg   \n",
       "4  Entradas e saídas de uma função [SEP] Entenda ...             pt   \n",
       "\n",
       "                       topic_title              id  \\\n",
       "0       Откриването на резисторите  c_1108dd0c7a5d   \n",
       "1       Откриването на резисторите  c_376c5a8eb028   \n",
       "2       Откриването на резисторите  c_5bc0e1e2cba0   \n",
       "3       Откриването на резисторите  c_76231f9d0b5e   \n",
       "4  Entradas e saídas de uma função  c_639ea2ef9c95   \n",
       "\n",
       "                                       content_title  \\\n",
       "0                                Молив като резистор   \n",
       "1                 Да чуем променливото съпротивление   \n",
       "2     Променлив резистор (реостат) с графит от молив   \n",
       "3  Последователно свързване на галваничен елемент...   \n",
       "4            Dados e resultados de funções: gráficos   \n",
       "\n",
       "                                   content_full_text content_language  label  \\\n",
       "0  Молив като резистор [SEP] Моливът причинява пр...               bg      1   \n",
       "1  Да чуем променливото съпротивление [SEP] Тук ч...               bg      1   \n",
       "2  Променлив резистор (реостат) с графит от молив...               bg      1   \n",
       "3  Последователно свързване на галваничен елемент...               bg      1   \n",
       "4  Dados e resultados de funções: gráficos [SEP] ...               pt      1   \n",
       "\n",
       "   fold  \n",
       "0    -1  \n",
       "1    -1  \n",
       "2    -1  \n",
       "3    -1  \n",
       "4    -1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_df = pd.read_parquet('random_negative_for_recall_exp4.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_df = neg_df.merge(fold_df, on='topic_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_df = neg_df[['topic_full_text', 'content_full_text', 'topic_id','label', 'fold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_full_text</th>\n",
       "      <th>content_full_text</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Откриването на резисторите [SEP] Изследване на...</td>\n",
       "      <td>Молив като резистор [SEP] Моливът причинява пр...</td>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Откриването на резисторите [SEP] Изследване на...</td>\n",
       "      <td>Да чуем променливото съпротивление [SEP] Тук ч...</td>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Откриването на резисторите [SEP] Изследване на...</td>\n",
       "      <td>Променлив резистор (реостат) с графит от молив...</td>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Откриването на резисторите [SEP] Изследване на...</td>\n",
       "      <td>Последователно свързване на галваничен елемент...</td>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entradas e saídas de uma função [SEP] Entenda ...</td>\n",
       "      <td>Dados e resultados de funções: gráficos [SEP] ...</td>\n",
       "      <td>t_00068291e9a4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     topic_full_text  \\\n",
       "0  Откриването на резисторите [SEP] Изследване на...   \n",
       "1  Откриването на резисторите [SEP] Изследване на...   \n",
       "2  Откриването на резисторите [SEP] Изследване на...   \n",
       "3  Откриването на резисторите [SEP] Изследване на...   \n",
       "4  Entradas e saídas de uma função [SEP] Entenda ...   \n",
       "\n",
       "                                   content_full_text        topic_id  label  \\\n",
       "0  Молив като резистор [SEP] Моливът причинява пр...  t_00004da3a1b2      1   \n",
       "1  Да чуем променливото съпротивление [SEP] Тук ч...  t_00004da3a1b2      1   \n",
       "2  Променлив резистор (реостат) с графит от молив...  t_00004da3a1b2      1   \n",
       "3  Последователно свързване на галваничен елемент...  t_00004da3a1b2      1   \n",
       "4  Dados e resultados de funções: gráficos [SEP] ...  t_00068291e9a4      1   \n",
       "\n",
       "   fold  \n",
       "0    -1  \n",
       "1    -1  \n",
       "2    -1  \n",
       "3    -1  \n",
       "4    -1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['topic_full_text', 'content_full_text', 'topic_id','label', 'fold']]\n",
    "df = pd.concat([df, neg_df])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    for punctuation in list(string.punctuation): text = text.replace(punctuation, '')\n",
    "    output = re.sub('\\r+', ' ', text)\n",
    "    output = re.sub('\\n+', ' ', output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_parquet('train_fold_recall_exp1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_parquet('train_fold_recall_exp3.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neg_df.to_parquet('random_negative_for_recall_exp1.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jsi4Pr8wWqM"
   },
   "source": [
    "## create CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Vu2P55dhwp5G"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "from transformers import BertTokenizer,AutoModel,AdamW,AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "import random\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "CbJxrkWxxWti"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    input_path = '/root/autodl-tmp/'\n",
    "    model_path = 'xlm-roberta-base' \n",
    "    exp_name = 'recall_exp4'\n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5  # 1.5\n",
    "    num_warmup_steps = 0.1\n",
    "    max_input_length = 256\n",
    "    epochs = 5  # 5\n",
    "    encoder_lr = 20e-6\n",
    "    decoder_lr = 1e-3\n",
    "    min_lr = 0.5e-6\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    weight_decay = 0\n",
    "    num_fold = 5\n",
    "    batch_size = 128\n",
    "    seed = 1006\n",
    "    OUTPUT_DIR = '/root/autodl-tmp/'\n",
    "    num_workers = 2\n",
    "    device='cuda'\n",
    "    print_freq = 100\n",
    "    apex=False\n",
    "    start_awp_epoch = 2 # 开始AWP epoch\n",
    "    adv_lr = 1e-5 # AWP学习率\n",
    "    adv_eps = 1e-3 # AWP epsilon\n",
    "    adv_step = 1 # AWP step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "safR828DxavN"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "j1-oQIqpwalb"
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer):\n",
    "        self.topic = df['topic_full_text'].values\n",
    "        self.content = df['content_full_text'].values\n",
    "        self.label = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "    def __len__(self):\n",
    "        return len(self.topic)\n",
    "    def __getitem__(self, item):\n",
    "        topic = self.topic[item].replace('[SEP]', self.sep_token)\n",
    "        content = self.content[item].replace('[SEP]', self.sep_token)\n",
    "        label = int(self.label[item])\n",
    "\n",
    "        \n",
    "        inputs_topic = self.tokenizer(topic, truncation=True, max_length=CFG.max_input_length, padding='max_length')\n",
    "        inputs_content = self.tokenizer(content, truncation=True, max_length=CFG.max_input_length, padding='max_length')\n",
    "        return torch.as_tensor(inputs_topic['input_ids'], dtype=torch.long), \\\n",
    "            torch.as_tensor(inputs_topic['attention_mask'], dtype=torch.long), \\\n",
    "            torch.as_tensor(inputs_content['input_ids'], dtype=torch.long), \\\n",
    "            torch.as_tensor(inputs_content['attention_mask'], dtype=torch.long), \\\n",
    "            torch.as_tensor(label, dtype=torch.float)\n",
    "    \n",
    "    \n",
    "class TopicTestDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer):\n",
    "        self.input = df['topic_full_text'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        input_text = self.input[item]\n",
    "        output = self.tokenizer(input_text, truncation=True, max_length=CFG.max_input_length, padding='max_length')\n",
    "        \n",
    "        return torch.as_tensor(output['input_ids'], dtype=torch.long), \\\n",
    "            torch.as_tensor(output['attention_mask'], dtype=torch.long)\n",
    "    \n",
    "class ContentTestDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer):\n",
    "        self.input = df['content_full_text'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        input_text = self.input[item]\n",
    "        output = self.tokenizer(input_text, truncation=True, max_length=CFG.max_input_length, padding='max_length')\n",
    "        \n",
    "        return torch.as_tensor(output['input_ids'], dtype=torch.long), \\\n",
    "            torch.as_tensor(output['attention_mask'], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnCXK1kYzPCo"
   },
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "i0w2WY_OzLeC"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "class Custom_Bert_Simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path)\n",
    "        self.config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(self.config.hidden_size*3, 1)\n",
    "\n",
    "    def forward(self,\n",
    "        topic_input_ids,\n",
    "        content_input_ids,\n",
    "        topic_attention_mask=None,\n",
    "        content_attention_mask=None, \n",
    "        labels=None):\n",
    "        topic_output = self.base(input_ids=topic_input_ids,attention_mask=topic_attention_mask)\n",
    "        topic_output = self.drop(topic_output.last_hidden_state)\n",
    "        topic_output_mask = torch.unsqueeze(topic_attention_mask, 2)\n",
    "        #print(topic_output_mask)\n",
    "        topic_output *= topic_output_mask\n",
    "        topic_output = torch.sum(topic_output, dim=1)\n",
    "        topic_output_mask = torch.sum(topic_output_mask, dim=1)\n",
    "        topic_output /= topic_output_mask\n",
    "        #print(topic_output)\n",
    "        \n",
    "        content_output = self.base(input_ids=content_input_ids,attention_mask=content_attention_mask)\n",
    "        content_output = self.drop(content_output.last_hidden_state)\n",
    "        content_output_mask = torch.unsqueeze(content_attention_mask, 2)\n",
    "        content_output *= content_output_mask\n",
    "        content_output = torch.sum(content_output, dim=1)\n",
    "        content_output_mask = torch.sum(content_output_mask, dim=1)\n",
    "        content_output /= content_output_mask\n",
    "        \n",
    "        \n",
    "        diff = torch.abs(topic_output-content_output)\n",
    "        sentence_embedding = torch.cat([topic_output, content_output, diff], 1)\n",
    "\n",
    "        output = self.linear(sentence_embedding)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.binary_cross_entropy_with_logits(output.view(-1), labels.view(-1))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Bert_SimCSE(nn.Module):\n",
    "    def __init__(self, margin=0.3, scale=30):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path)\n",
    "        self.config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        self.margin = margin\n",
    "        # Used scaling cosine similarity to ease converge\n",
    "        self.sacle = scale\n",
    "\n",
    "    def forward(self,\n",
    "        topic_input_ids,\n",
    "        content_input_ids,\n",
    "        topic_attention_mask=None,\n",
    "        content_attention_mask=None, \n",
    "        labels=None):\n",
    "        topic_output = self.base(input_ids=topic_input_ids,attention_mask=topic_attention_mask)\n",
    "        topic_output = topic_output.last_hidden_state\n",
    "        topic_output = torch.mean(topic_output, dim=1)\n",
    "\n",
    "        content_output = self.base(input_ids=content_input_ids,attention_mask=content_attention_mask)\n",
    "        content_output = content_output.last_hidden_state\n",
    "        content_output = torch.mean(content_output, dim=1)\n",
    "        \n",
    "        # topic: 64 * 768\n",
    "        # content: 64 * 768\n",
    "        cosine_sim = torch.matmul(topic_output, content_output.T)\n",
    "        # 64 * 64 --> batch * batch 其中 cosine_sim[i][j] 第i个topic和第j个content\n",
    "\n",
    "        # substract margin from all positive samples cosine_sim()\n",
    "        margin_diag = torch.full(\n",
    "            [topic_output.shape[0]], fill_value=self.margin\n",
    "        )\n",
    "        # margin_diag --》batch * batch 元素全是fill_value的一个矩阵\n",
    "        \n",
    "        cosine_sim = cosine_sim - torch.diag(margin_diag).to(CFG.device)\n",
    "\n",
    "        # scale cosine to ease training converge\n",
    "        cosine_sim *= self.sacle\n",
    "        \n",
    "        ## batch * batch \n",
    "\n",
    "        labels = torch.arange(0, topic_output.shape[0])\n",
    "        labels = torch.reshape(labels, shape=[-1]).to(CFG.device)\n",
    "        \n",
    "        # print(cosine_sim.shape)\n",
    "        # print(labels.shape)\n",
    "        \n",
    "        \n",
    "        loss = F.cross_entropy(cosine_sim, labels)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Bert_SimCSE_Test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path)\n",
    "        self.config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "\n",
    "    def forward(self,\n",
    "        input_ids,\n",
    "        attention_mask=None):\n",
    "        output = self.base(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        output = output.last_hidden_state\n",
    "        output = torch.mean(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Bert_Simple_Test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path)\n",
    "        self.config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(self.config.hidden_size*3, 1)\n",
    "\n",
    "    def forward(self,\n",
    "        input_ids,\n",
    "        attention_mask=None):\n",
    "        output = self.base(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        output = self.drop(output.last_hidden_state)\n",
    "        attention_mask = torch.unsqueeze(attention_mask, 2)\n",
    "        #print(topic_output_mask)\n",
    "        output *= attention_mask\n",
    "        output = torch.sum(output, dim=1)\n",
    "        attention_mask = torch.sum(attention_mask, dim=1)\n",
    "        output /= attention_mask\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xB4GaI9fDzt1"
   },
   "source": [
    "## build logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "R3SPEdriD2lE"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6VVH3jSEYie",
    "outputId": "b0d4793c-ef1e-4623-b4f7-9a4f6ce628cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===============lr_2e-05===============\n",
      "===============seed_1006===============\n",
      "===============total_epochs_5===============\n",
      "===============num_warmup_steps_0.1===============\n"
     ]
    }
   ],
   "source": [
    "def get_logger(filename=CFG.OUTPUT_DIR+ 'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "LOGGER.info('===============lr_{}==============='.format(CFG.encoder_lr))\n",
    "LOGGER.info('===============seed_{}==============='.format(CFG.seed))\n",
    "LOGGER.info('===============total_epochs_{}==============='.format(CFG.epochs))\n",
    "LOGGER.info('===============num_warmup_steps_{}==============='.format(CFG.num_warmup_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQDtzCGV5S0B"
   },
   "source": [
    "## build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(embeddings, ids):\n",
    "\n",
    "    index = hnswlib.Index(space=\"cosine\", dim=embeddings.shape[-1])\n",
    "\n",
    "    # Initializing index\n",
    "    # max_elements - the maximum number of elements (capacity). Will throw an exception if exceeded\n",
    "    # during insertion of an element.\n",
    "    # The capacity can be increased by saving/loading the index, see below.\n",
    "    #\n",
    "    # ef_construction - controls index search speed/build speed tradeoff\n",
    "    #\n",
    "    # M - is tightly connected with internal dimensionality of the data. Strongly affects memory consumption (~M)\n",
    "    # Higher M leads to higher accuracy/run_time at fixed ef/efConstruction\n",
    "    index.init_index(max_elements=embeddings.shape[0], ef_construction=200, M=1000)\n",
    "\n",
    "    # Controlling the recall by setting ef:\n",
    "    # higher ef leads to better accuracy, but slower search\n",
    "    index.set_ef(1000)\n",
    "\n",
    "    # Set number of threads used during batch search/construction\n",
    "    # By default using all available cores\n",
    "    index.set_num_threads(16)\n",
    "\n",
    "    \n",
    "    index.add_items(embeddings, ids)\n",
    "\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "S6Pg-_675VB2"
   },
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = [i.to(device) for i in batch]\n",
    "        topic_input_ids, topic_attention_mask, content_input_ids, content_attention_mask, label = batch\n",
    "        batch_size = label.size(0)\n",
    "        loss = model(topic_input_ids, content_input_ids, topic_attention_mask, content_attention_mask, label)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 500)\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def infer(model, dataloader):\n",
    "    res = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids, attention_mask = [i.to(CFG.device) for i in batch]\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, attention_mask)\n",
    "            res.append(output.cpu().numpy())\n",
    "\n",
    "    return np.vstack(res)\n",
    "\n",
    "\n",
    "def recall(targets, preds): return len([x for x in targets if x in preds])/(len(targets)+ 1e-16)\n",
    "\n",
    "def f2_score(y_true, y_pred):\n",
    "\n",
    "    y_true = [set(i.split()) for i in y_true]\n",
    "    y_pred = [set(i.split()) for i in y_pred]\n",
    "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n",
    "    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n",
    "    precision = tp / (tp + fp)\n",
    "    recs = [recall(t,p) for t,p in list(zip(y_true, y_pred))]\n",
    "    f2 = tp / (tp + 0.2 * fp + 0.8 * fn)\n",
    "    return round(f2.mean(), 4), np.nanmean(recs)\n",
    "\n",
    "\n",
    "def valid_fn(val_df, tokenizer):\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "    val_topic_id = val_df['topic_id'].unique().tolist()\n",
    "    content_df = pd.read_csv('content.csv')\n",
    "    content_df = content_df.fillna('')\n",
    "    content_df['content_full_text'] = content_df['title'] + ' [SEP] ' + content_df['description'] + ' [SEP] ' + content_df['text']\n",
    "    topic_df = pd.read_csv('topics.csv')\n",
    "    topic_df = topic_df[topic_df['id'].isin(val_topic_id)]\n",
    "    topic_df = topic_df.fillna('')\n",
    "    topic_df['topic_full_text'] = topic_df['title'] + ' [SEP] ' + topic_df['description']\n",
    "    topic_dataset = TopicTestDataset(topic_df, tokenizer)\n",
    "    content_dataset = ContentTestDataset(content_df, tokenizer)\n",
    "    topic_loader = DataLoader(topic_dataset,\n",
    "                                  batch_size=CFG.batch_size * 2,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    content_loader = DataLoader(content_dataset,\n",
    "                                  batch_size=CFG.batch_size * 2,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    model = Custom_Bert_Simple_Test()\n",
    "    model.load_state_dict(torch.load(CFG.OUTPUT_DIR + \"{}_{}_best{}_tmp.pth\".format(CFG.model_path.replace('/', '_'),CFG.exp_name,fold)),strict=False)\n",
    "    model.to(CFG.device)\n",
    "    model.eval()\n",
    "    topic_result = infer(model, topic_loader)\n",
    "    content_result = infer(model, content_loader)\n",
    "    content_ids = [i for i in range(len(content_df))]\n",
    "    content_index = build_index(content_result, content_ids)\n",
    "    results = content_index.knn_query(topic_result, k = 5, num_threads = -1)\n",
    "    pred = []\n",
    "    content_uid = content_df['id']\n",
    "    for result in tqdm(results[0]):\n",
    "        top_same = ' '.join(content_uid[result].to_list())\n",
    "        pred.append(top_same)\n",
    "    corr_df_init = pd.read_csv('correlations.csv')\n",
    "    corr_df_init = corr_df_init[corr_df_init['topic_id'].isin(val_topic_id)]\n",
    "    gts = topic_df.merge(corr_df_init, how='left', left_on='id', right_on='topic_id')['content_ids'].to_list()                                 \n",
    "    score, recall = f2_score(gts, pred)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    return score, recall\n",
    "\n",
    "def train_loop(fold, model, train_dataset, va_data):\n",
    "    LOGGER.info(f\"========== training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    #model = Custom_Bert_Simple()\n",
    "    #model = AutoModelForSequenceClassification.from_pretrained(CFG.model_path, num_labels=1)\n",
    "    model.to(CFG.device)\n",
    "\n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    def get_optimizer(model):\n",
    "\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                'lr': CFG.encoder_lr, 'weight_decay': CFG.weight_decay},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                'lr': CFG.encoder_lr, 'weight_decay': 0.0}\n",
    "            \n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_parameters, lr = CFG.encoder_lr, eps = CFG.eps, betas = CFG.betas)\n",
    "        return optimizer\n",
    "\n",
    "    \n",
    "    optimizer = get_optimizer(model)\n",
    "\n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        cfg.num_warmup_steps = cfg.num_warmup_steps * num_train_steps\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps,\n",
    "                num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "\n",
    "    num_train_steps = int(len(train_dataset) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    # criterion = torch.nn.CrossEntropyLoss(ignore_index=- 1)\n",
    "\n",
    "    # criterion = LabelSmoothingLoss()\n",
    "    best_score = 0\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        #avg_loss = 0.01#train_fn_awp(train_loader, model, optimizer, epoch, scheduler, CFG.device)\n",
    "        \n",
    "        avg_loss = train_fn(train_loader, model, optimizer, epoch, scheduler, CFG.device)\n",
    "        # eval\n",
    "        torch.save(model.state_dict(),\n",
    "                       CFG.OUTPUT_DIR + \"{}_{}_best{}_tmp.pth\".format(CFG.model_path.replace('/', '_'),CFG.exp_name,fold))\n",
    "        \n",
    "        score, recall = valid_fn(va_data, tokenizer)\n",
    "\n",
    "        # scoring\n",
    "        #score = get_score(predictions, valid_labels)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(\n",
    "            f'Epoch {epoch + 1} - avg_train_loss: {avg_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch + 1} - Score: {score:.4f} - Recall:{recall:.4f}')\n",
    "        \n",
    "        \n",
    "\n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            #best_predictions = predictions\n",
    "            LOGGER.info(f'Epoch {epoch + 1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save(model.state_dict(),\n",
    "                       CFG.OUTPUT_DIR + \"{}_{}_best{}.pth\".format(CFG.model_path.replace('/', '_'),CFG.exp_name,fold))\n",
    "\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    del scheduler, optimizer, model\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kazL85iWEb5W",
    "outputId": "5953d979-3e8b-4911-8876-834e16ceb357",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Custom_Bert_Simple()\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n",
    "fold = 0\n",
    "tr_data = df[df['fold']!=fold].reset_index(drop=True)\n",
    "va_data = df[df['fold']==fold].reset_index(drop=True)\n",
    "tr_dataset = TrainDataset(tr_data,tokenizer)\n",
    "va_dataset = TrainDataset(va_data,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== training ==========\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/41910] Elapsed 0m 3s (remain 2182m 21s) Loss: 0.7683(0.7683) Grad: 13.0857  LR: 0.00000000  \n",
      "Epoch: [1][100/41910] Elapsed 1m 15s (remain 523m 30s) Loss: 0.7702(0.7685) Grad: 12.6939  LR: 0.00000010  \n",
      "Epoch: [1][200/41910] Elapsed 2m 28s (remain 513m 33s) Loss: 0.7298(0.7597) Grad: 11.6402  LR: 0.00000019  \n",
      "Epoch: [1][300/41910] Elapsed 3m 41s (remain 509m 38s) Loss: 0.6841(0.7431) Grad: 12.0506  LR: 0.00000029  \n",
      "Epoch: [1][400/41910] Elapsed 4m 53s (remain 507m 2s) Loss: 0.5979(0.7187) Grad: 10.7986  LR: 0.00000038  \n",
      "Epoch: [1][500/41910] Elapsed 6m 6s (remain 504m 54s) Loss: 0.4241(0.6797) Grad: 7.5669  LR: 0.00000048  \n",
      "Epoch: [1][600/41910] Elapsed 7m 19s (remain 503m 8s) Loss: 0.1288(0.6064) Grad: 2.0316  LR: 0.00000057  \n",
      "Epoch: [1][700/41910] Elapsed 8m 31s (remain 501m 31s) Loss: 0.1922(0.5480) Grad: 2.2735  LR: 0.00000067  \n",
      "Epoch: [1][800/41910] Elapsed 9m 44s (remain 500m 3s) Loss: 0.1445(0.5022) Grad: 0.9380  LR: 0.00000076  \n",
      "Epoch: [1][900/41910] Elapsed 10m 57s (remain 498m 33s) Loss: 0.2624(0.4673) Grad: 1.6332  LR: 0.00000086  \n",
      "Epoch: [1][1000/41910] Elapsed 12m 9s (remain 497m 7s) Loss: 0.1903(0.4409) Grad: 1.4449  LR: 0.00000096  \n",
      "Epoch: [1][1100/41910] Elapsed 13m 22s (remain 495m 47s) Loss: 0.1648(0.4178) Grad: 2.9421  LR: 0.00000105  \n",
      "Epoch: [1][1200/41910] Elapsed 14m 35s (remain 494m 28s) Loss: 0.1007(0.3997) Grad: 1.7585  LR: 0.00000115  \n",
      "Epoch: [1][1300/41910] Elapsed 15m 47s (remain 493m 3s) Loss: 0.2135(0.3840) Grad: 1.2687  LR: 0.00000124  \n",
      "Epoch: [1][1400/41910] Elapsed 17m 0s (remain 491m 47s) Loss: 0.2299(0.3708) Grad: 1.0878  LR: 0.00000134  \n",
      "Epoch: [1][1500/41910] Elapsed 18m 13s (remain 490m 27s) Loss: 0.1871(0.3591) Grad: 1.0967  LR: 0.00000143  \n",
      "Epoch: [1][1600/41910] Elapsed 19m 25s (remain 489m 12s) Loss: 0.2859(0.3488) Grad: 10.6354  LR: 0.00000153  \n",
      "Epoch: [1][1700/41910] Elapsed 20m 38s (remain 487m 54s) Loss: 0.2556(0.3402) Grad: 1.0054  LR: 0.00000162  \n",
      "Epoch: [1][1800/41910] Elapsed 21m 51s (remain 486m 40s) Loss: 0.1425(0.3316) Grad: 1.1555  LR: 0.00000172  \n",
      "Epoch: [1][1900/41910] Elapsed 23m 3s (remain 485m 23s) Loss: 0.2142(0.3240) Grad: 1.6608  LR: 0.00000181  \n",
      "Epoch: [1][2000/41910] Elapsed 24m 16s (remain 484m 8s) Loss: 0.2210(0.3173) Grad: 1.6529  LR: 0.00000191  \n",
      "Epoch: [1][2100/41910] Elapsed 25m 29s (remain 482m 55s) Loss: 0.2031(0.3113) Grad: 1.1162  LR: 0.00000201  \n",
      "Epoch: [1][2200/41910] Elapsed 26m 41s (remain 481m 38s) Loss: 0.2854(0.3058) Grad: 8.8015  LR: 0.00000210  \n",
      "Epoch: [1][2300/41910] Elapsed 27m 54s (remain 480m 24s) Loss: 0.1139(0.3002) Grad: 0.9235  LR: 0.00000220  \n",
      "Epoch: [1][2400/41910] Elapsed 29m 7s (remain 479m 9s) Loss: 0.1616(0.2954) Grad: 1.2641  LR: 0.00000229  \n",
      "Epoch: [1][2500/41910] Elapsed 30m 19s (remain 477m 55s) Loss: 0.2564(0.2908) Grad: 1.8323  LR: 0.00000239  \n",
      "Epoch: [1][2600/41910] Elapsed 31m 32s (remain 476m 41s) Loss: 0.2690(0.2865) Grad: 2.0690  LR: 0.00000248  \n",
      "Epoch: [1][2700/41910] Elapsed 32m 45s (remain 475m 28s) Loss: 0.0984(0.2827) Grad: 1.4931  LR: 0.00000258  \n",
      "Epoch: [1][2800/41910] Elapsed 33m 57s (remain 474m 11s) Loss: 0.1053(0.2792) Grad: 1.3458  LR: 0.00000267  \n",
      "Epoch: [1][2900/41910] Elapsed 35m 10s (remain 472m 56s) Loss: 0.0698(0.2759) Grad: 1.9171  LR: 0.00000277  \n",
      "Epoch: [1][3000/41910] Elapsed 36m 23s (remain 471m 43s) Loss: 0.1735(0.2727) Grad: 1.3889  LR: 0.00000286  \n",
      "Epoch: [1][3100/41910] Elapsed 37m 35s (remain 470m 30s) Loss: 0.1786(0.2697) Grad: 1.6555  LR: 0.00000296  \n",
      "Epoch: [1][3200/41910] Elapsed 38m 48s (remain 469m 15s) Loss: 0.1541(0.2669) Grad: 2.0123  LR: 0.00000306  \n",
      "Epoch: [1][3300/41910] Elapsed 40m 0s (remain 468m 0s) Loss: 0.1713(0.2644) Grad: 1.4167  LR: 0.00000315  \n",
      "Epoch: [1][3400/41910] Elapsed 41m 13s (remain 466m 47s) Loss: 0.2286(0.2616) Grad: 2.5933  LR: 0.00000325  \n",
      "Epoch: [1][3500/41910] Elapsed 42m 26s (remain 465m 34s) Loss: 0.2060(0.2590) Grad: 2.1560  LR: 0.00000334  \n",
      "Epoch: [1][3600/41910] Elapsed 43m 38s (remain 464m 20s) Loss: 0.1690(0.2562) Grad: 1.8681  LR: 0.00000344  \n",
      "Epoch: [1][3700/41910] Elapsed 44m 51s (remain 463m 7s) Loss: 0.3212(0.2536) Grad: 3.3187  LR: 0.00000353  \n",
      "Epoch: [1][3800/41910] Elapsed 46m 4s (remain 461m 54s) Loss: 0.1152(0.2514) Grad: 1.5133  LR: 0.00000363  \n",
      "Epoch: [1][3900/41910] Elapsed 47m 17s (remain 460m 42s) Loss: 0.1667(0.2492) Grad: 2.8238  LR: 0.00000372  \n",
      "Epoch: [1][4000/41910] Elapsed 48m 29s (remain 459m 29s) Loss: 0.1552(0.2468) Grad: 1.4490  LR: 0.00000382  \n",
      "Epoch: [1][4100/41910] Elapsed 49m 42s (remain 458m 15s) Loss: 0.1807(0.2448) Grad: 2.1113  LR: 0.00000391  \n",
      "Epoch: [1][4200/41910] Elapsed 50m 55s (remain 457m 3s) Loss: 0.2274(0.2427) Grad: 2.2410  LR: 0.00000401  \n",
      "Epoch: [1][4300/41910] Elapsed 52m 7s (remain 455m 50s) Loss: 0.1878(0.2407) Grad: 2.2059  LR: 0.00000410  \n",
      "Epoch: [1][4400/41910] Elapsed 53m 20s (remain 454m 38s) Loss: 0.2375(0.2388) Grad: 3.0781  LR: 0.00000420  \n",
      "Epoch: [1][4500/41910] Elapsed 54m 33s (remain 453m 25s) Loss: 0.0890(0.2368) Grad: 1.0731  LR: 0.00000430  \n",
      "Epoch: [1][4600/41910] Elapsed 55m 45s (remain 452m 10s) Loss: 0.2575(0.2348) Grad: 3.5593  LR: 0.00000439  \n",
      "Epoch: [1][4700/41910] Elapsed 56m 58s (remain 450m 57s) Loss: 0.1241(0.2328) Grad: 2.0084  LR: 0.00000449  \n",
      "Epoch: [1][4800/41910] Elapsed 58m 11s (remain 449m 43s) Loss: 0.1592(0.2311) Grad: 1.6885  LR: 0.00000458  \n",
      "Epoch: [1][4900/41910] Elapsed 59m 23s (remain 448m 30s) Loss: 0.0861(0.2292) Grad: 1.0972  LR: 0.00000468  \n",
      "Epoch: [1][5000/41910] Elapsed 60m 36s (remain 447m 18s) Loss: 0.1770(0.2275) Grad: 2.1659  LR: 0.00000477  \n",
      "Epoch: [1][5100/41910] Elapsed 61m 49s (remain 446m 4s) Loss: 0.1605(0.2257) Grad: 3.5084  LR: 0.00000487  \n",
      "Epoch: [1][5200/41910] Elapsed 63m 1s (remain 444m 51s) Loss: 0.1569(0.2242) Grad: 2.7407  LR: 0.00000496  \n",
      "Epoch: [1][5300/41910] Elapsed 64m 14s (remain 443m 38s) Loss: 0.1596(0.2226) Grad: 2.2909  LR: 0.00000506  \n",
      "Epoch: [1][5400/41910] Elapsed 65m 27s (remain 442m 25s) Loss: 0.0681(0.2210) Grad: 1.7600  LR: 0.00000515  \n",
      "Epoch: [1][5500/41910] Elapsed 66m 39s (remain 441m 12s) Loss: 0.1028(0.2195) Grad: 2.1093  LR: 0.00000525  \n",
      "Epoch: [1][5600/41910] Elapsed 67m 52s (remain 439m 58s) Loss: 0.1681(0.2180) Grad: 2.7107  LR: 0.00000535  \n",
      "Epoch: [1][5700/41910] Elapsed 69m 4s (remain 438m 45s) Loss: 0.2259(0.2165) Grad: 3.2361  LR: 0.00000544  \n",
      "Epoch: [1][5800/41910] Elapsed 70m 17s (remain 437m 32s) Loss: 0.0734(0.2150) Grad: 1.5197  LR: 0.00000554  \n",
      "Epoch: [1][5900/41910] Elapsed 71m 30s (remain 436m 19s) Loss: 0.1085(0.2135) Grad: 1.5805  LR: 0.00000563  \n",
      "Epoch: [1][6000/41910] Elapsed 72m 42s (remain 435m 6s) Loss: 0.1450(0.2122) Grad: 2.3186  LR: 0.00000573  \n",
      "Epoch: [1][6100/41910] Elapsed 73m 55s (remain 433m 53s) Loss: 0.1788(0.2108) Grad: 4.0177  LR: 0.00000582  \n",
      "Epoch: [1][6200/41910] Elapsed 75m 8s (remain 432m 40s) Loss: 0.1135(0.2096) Grad: 1.7113  LR: 0.00000592  \n",
      "Epoch: [1][6300/41910] Elapsed 76m 20s (remain 431m 27s) Loss: 0.0976(0.2083) Grad: 2.0746  LR: 0.00000601  \n",
      "Epoch: [1][6400/41910] Elapsed 77m 33s (remain 430m 13s) Loss: 0.1002(0.2070) Grad: 1.8037  LR: 0.00000611  \n",
      "Epoch: [1][6500/41910] Elapsed 78m 45s (remain 429m 0s) Loss: 0.1605(0.2057) Grad: 2.7595  LR: 0.00000620  \n",
      "Epoch: [1][6600/41910] Elapsed 79m 58s (remain 427m 48s) Loss: 0.0960(0.2044) Grad: 1.5641  LR: 0.00000630  \n",
      "Epoch: [1][6700/41910] Elapsed 81m 11s (remain 426m 36s) Loss: 0.1428(0.2031) Grad: 2.5618  LR: 0.00000640  \n",
      "Epoch: [1][6800/41910] Elapsed 82m 24s (remain 425m 23s) Loss: 0.1228(0.2019) Grad: 1.7563  LR: 0.00000649  \n",
      "Epoch: [1][6900/41910] Elapsed 83m 36s (remain 424m 11s) Loss: 0.1700(0.2007) Grad: 2.6357  LR: 0.00000659  \n",
      "Epoch: [1][7000/41910] Elapsed 84m 49s (remain 422m 58s) Loss: 0.0878(0.1995) Grad: 3.0326  LR: 0.00000668  \n",
      "Epoch: [1][7100/41910] Elapsed 86m 2s (remain 421m 45s) Loss: 0.0566(0.1984) Grad: 1.3547  LR: 0.00000678  \n",
      "Epoch: [1][7200/41910] Elapsed 87m 15s (remain 420m 33s) Loss: 0.1835(0.1972) Grad: 2.6508  LR: 0.00000687  \n",
      "Epoch: [1][7300/41910] Elapsed 88m 27s (remain 419m 20s) Loss: 0.1116(0.1961) Grad: 2.1387  LR: 0.00000697  \n",
      "Epoch: [1][7400/41910] Elapsed 89m 40s (remain 418m 8s) Loss: 0.1306(0.1950) Grad: 2.7789  LR: 0.00000706  \n",
      "Epoch: [1][7500/41910] Elapsed 90m 53s (remain 416m 55s) Loss: 0.1693(0.1939) Grad: 2.9425  LR: 0.00000716  \n",
      "Epoch: [1][7600/41910] Elapsed 92m 5s (remain 415m 42s) Loss: 0.0998(0.1928) Grad: 2.1960  LR: 0.00000725  \n",
      "Epoch: [1][7700/41910] Elapsed 93m 18s (remain 414m 30s) Loss: 0.1627(0.1916) Grad: 3.4532  LR: 0.00000735  \n",
      "Epoch: [1][7800/41910] Elapsed 94m 31s (remain 413m 17s) Loss: 0.1004(0.1906) Grad: 1.9747  LR: 0.00000745  \n",
      "Epoch: [1][7900/41910] Elapsed 95m 44s (remain 412m 5s) Loss: 0.1275(0.1894) Grad: 3.1059  LR: 0.00000754  \n",
      "Epoch: [1][8000/41910] Elapsed 96m 56s (remain 410m 52s) Loss: 0.1344(0.1884) Grad: 1.8346  LR: 0.00000764  \n",
      "Epoch: [1][8100/41910] Elapsed 98m 9s (remain 409m 39s) Loss: 0.0915(0.1874) Grad: 2.1321  LR: 0.00000773  \n",
      "Epoch: [1][8200/41910] Elapsed 99m 22s (remain 408m 26s) Loss: 0.1229(0.1864) Grad: 4.1702  LR: 0.00000783  \n",
      "Epoch: [1][8300/41910] Elapsed 100m 34s (remain 407m 13s) Loss: 0.1860(0.1854) Grad: 4.3075  LR: 0.00000792  \n",
      "Epoch: [1][8400/41910] Elapsed 101m 47s (remain 406m 1s) Loss: 0.0847(0.1844) Grad: 1.7722  LR: 0.00000802  \n",
      "Epoch: [1][8500/41910] Elapsed 103m 0s (remain 404m 48s) Loss: 0.0587(0.1834) Grad: 1.3933  LR: 0.00000811  \n",
      "Epoch: [1][8600/41910] Elapsed 104m 12s (remain 403m 35s) Loss: 0.0643(0.1825) Grad: 1.6549  LR: 0.00000821  \n",
      "Epoch: [1][8700/41910] Elapsed 105m 25s (remain 402m 23s) Loss: 0.1022(0.1816) Grad: 2.3805  LR: 0.00000830  \n",
      "Epoch: [1][8800/41910] Elapsed 106m 38s (remain 401m 12s) Loss: 0.1220(0.1807) Grad: 2.4455  LR: 0.00000840  \n",
      "Epoch: [1][8900/41910] Elapsed 107m 52s (remain 400m 1s) Loss: 0.1240(0.1799) Grad: 4.9038  LR: 0.00000850  \n",
      "Epoch: [1][9000/41910] Elapsed 109m 4s (remain 398m 48s) Loss: 0.1352(0.1791) Grad: 2.8812  LR: 0.00000859  \n",
      "Epoch: [1][9100/41910] Elapsed 110m 17s (remain 397m 35s) Loss: 0.0909(0.1782) Grad: 2.4164  LR: 0.00000869  \n",
      "Epoch: [1][9200/41910] Elapsed 111m 30s (remain 396m 23s) Loss: 0.0729(0.1773) Grad: 1.8723  LR: 0.00000878  \n",
      "Epoch: [1][9300/41910] Elapsed 112m 42s (remain 395m 9s) Loss: 0.0619(0.1765) Grad: 2.1943  LR: 0.00000888  \n",
      "Epoch: [1][9400/41910] Elapsed 113m 55s (remain 393m 58s) Loss: 0.0875(0.1756) Grad: 1.9368  LR: 0.00000897  \n",
      "Epoch: [1][9500/41910] Elapsed 115m 8s (remain 392m 44s) Loss: 0.1198(0.1748) Grad: 2.5050  LR: 0.00000907  \n",
      "Epoch: [1][9600/41910] Elapsed 116m 20s (remain 391m 31s) Loss: 0.0893(0.1739) Grad: 1.9296  LR: 0.00000916  \n",
      "Epoch: [1][9700/41910] Elapsed 117m 33s (remain 390m 18s) Loss: 0.0636(0.1731) Grad: 2.2913  LR: 0.00000926  \n",
      "Epoch: [1][9800/41910] Elapsed 118m 46s (remain 389m 6s) Loss: 0.1011(0.1724) Grad: 2.3780  LR: 0.00000935  \n",
      "Epoch: [1][9900/41910] Elapsed 119m 58s (remain 387m 53s) Loss: 0.0738(0.1716) Grad: 2.5749  LR: 0.00000945  \n",
      "Epoch: [1][10000/41910] Elapsed 121m 11s (remain 386m 40s) Loss: 0.0566(0.1707) Grad: 2.5079  LR: 0.00000955  \n",
      "Epoch: [1][10100/41910] Elapsed 122m 24s (remain 385m 28s) Loss: 0.1091(0.1700) Grad: 3.8637  LR: 0.00000964  \n",
      "Epoch: [1][10200/41910] Elapsed 123m 37s (remain 384m 15s) Loss: 0.0899(0.1692) Grad: 1.4592  LR: 0.00000974  \n",
      "Epoch: [1][10300/41910] Elapsed 124m 49s (remain 383m 2s) Loss: 0.0905(0.1684) Grad: 2.1727  LR: 0.00000983  \n",
      "Epoch: [1][10400/41910] Elapsed 126m 2s (remain 381m 49s) Loss: 0.1177(0.1676) Grad: 3.3311  LR: 0.00000993  \n",
      "Epoch: [1][10500/41910] Elapsed 127m 15s (remain 380m 36s) Loss: 0.1763(0.1669) Grad: 3.6744  LR: 0.00001002  \n",
      "Epoch: [1][10600/41910] Elapsed 128m 27s (remain 379m 24s) Loss: 0.1352(0.1662) Grad: 2.5911  LR: 0.00001012  \n",
      "Epoch: [1][10700/41910] Elapsed 129m 40s (remain 378m 11s) Loss: 0.0687(0.1654) Grad: 1.8940  LR: 0.00001021  \n",
      "Epoch: [1][10800/41910] Elapsed 130m 53s (remain 376m 58s) Loss: 0.0746(0.1647) Grad: 2.1287  LR: 0.00001031  \n",
      "Epoch: [1][10900/41910] Elapsed 132m 6s (remain 375m 46s) Loss: 0.0508(0.1640) Grad: 1.9616  LR: 0.00001040  \n",
      "Epoch: [1][11000/41910] Elapsed 133m 18s (remain 374m 33s) Loss: 0.0752(0.1633) Grad: 2.1332  LR: 0.00001050  \n",
      "Epoch: [1][11100/41910] Elapsed 134m 31s (remain 373m 20s) Loss: 0.0340(0.1626) Grad: 1.2819  LR: 0.00001059  \n",
      "Epoch: [1][11200/41910] Elapsed 135m 44s (remain 372m 8s) Loss: 0.1810(0.1619) Grad: 3.8493  LR: 0.00001069  \n",
      "Epoch: [1][11300/41910] Elapsed 136m 56s (remain 370m 55s) Loss: 0.1061(0.1613) Grad: 2.9276  LR: 0.00001079  \n",
      "Epoch: [1][11400/41910] Elapsed 138m 9s (remain 369m 42s) Loss: 0.0723(0.1605) Grad: 1.6786  LR: 0.00001088  \n",
      "Epoch: [1][11500/41910] Elapsed 139m 22s (remain 368m 29s) Loss: 0.0423(0.1599) Grad: 1.5342  LR: 0.00001098  \n",
      "Epoch: [1][11600/41910] Elapsed 140m 34s (remain 367m 17s) Loss: 0.0909(0.1592) Grad: 1.6390  LR: 0.00001107  \n",
      "Epoch: [1][11700/41910] Elapsed 141m 47s (remain 366m 4s) Loss: 0.0747(0.1585) Grad: 1.6881  LR: 0.00001117  \n",
      "Epoch: [1][11800/41910] Elapsed 143m 0s (remain 364m 51s) Loss: 0.0687(0.1579) Grad: 1.9563  LR: 0.00001126  \n",
      "Epoch: [1][11900/41910] Elapsed 144m 12s (remain 363m 38s) Loss: 0.0449(0.1573) Grad: 1.2630  LR: 0.00001136  \n",
      "Epoch: [1][12000/41910] Elapsed 145m 25s (remain 362m 25s) Loss: 0.0925(0.1566) Grad: 2.0989  LR: 0.00001145  \n",
      "Epoch: [1][12100/41910] Elapsed 146m 38s (remain 361m 13s) Loss: 0.0942(0.1561) Grad: 2.3697  LR: 0.00001155  \n",
      "Epoch: [1][12200/41910] Elapsed 147m 50s (remain 360m 0s) Loss: 0.0961(0.1554) Grad: 2.4363  LR: 0.00001164  \n",
      "Epoch: [1][12300/41910] Elapsed 149m 3s (remain 358m 47s) Loss: 0.1351(0.1548) Grad: 2.4448  LR: 0.00001174  \n",
      "Epoch: [1][12400/41910] Elapsed 150m 16s (remain 357m 34s) Loss: 0.0924(0.1542) Grad: 1.8844  LR: 0.00001184  \n",
      "Epoch: [1][12500/41910] Elapsed 151m 28s (remain 356m 22s) Loss: 0.0199(0.1536) Grad: 1.0232  LR: 0.00001193  \n",
      "Epoch: [1][12600/41910] Elapsed 152m 41s (remain 355m 9s) Loss: 0.0898(0.1530) Grad: 2.4128  LR: 0.00001203  \n",
      "Epoch: [1][12700/41910] Elapsed 153m 54s (remain 353m 56s) Loss: 0.0804(0.1524) Grad: 1.4246  LR: 0.00001212  \n",
      "Epoch: [1][12800/41910] Elapsed 155m 7s (remain 352m 43s) Loss: 0.0600(0.1518) Grad: 2.5244  LR: 0.00001222  \n",
      "Epoch: [1][12900/41910] Elapsed 156m 19s (remain 351m 31s) Loss: 0.0405(0.1512) Grad: 1.7036  LR: 0.00001231  \n",
      "Epoch: [1][13000/41910] Elapsed 157m 32s (remain 350m 19s) Loss: 0.0749(0.1507) Grad: 2.2546  LR: 0.00001241  \n",
      "Epoch: [1][13100/41910] Elapsed 158m 45s (remain 349m 6s) Loss: 0.1080(0.1501) Grad: 2.7484  LR: 0.00001250  \n",
      "Epoch: [1][13200/41910] Elapsed 159m 58s (remain 347m 53s) Loss: 0.0392(0.1495) Grad: 1.3251  LR: 0.00001260  \n",
      "Epoch: [1][13300/41910] Elapsed 161m 11s (remain 346m 41s) Loss: 0.0435(0.1489) Grad: 1.9695  LR: 0.00001269  \n",
      "Epoch: [1][13400/41910] Elapsed 162m 23s (remain 345m 28s) Loss: 0.0756(0.1483) Grad: 1.6573  LR: 0.00001279  \n",
      "Epoch: [1][13500/41910] Elapsed 163m 36s (remain 344m 16s) Loss: 0.0385(0.1478) Grad: 1.9599  LR: 0.00001289  \n",
      "Epoch: [1][13600/41910] Elapsed 164m 49s (remain 343m 3s) Loss: 0.0397(0.1473) Grad: 1.2085  LR: 0.00001298  \n",
      "Epoch: [1][13700/41910] Elapsed 166m 1s (remain 341m 50s) Loss: 0.0400(0.1467) Grad: 1.2097  LR: 0.00001308  \n",
      "Epoch: [1][13800/41910] Elapsed 167m 14s (remain 340m 38s) Loss: 0.0388(0.1461) Grad: 1.2107  LR: 0.00001317  \n",
      "Epoch: [1][13900/41910] Elapsed 168m 27s (remain 339m 25s) Loss: 0.0439(0.1457) Grad: 1.9554  LR: 0.00001327  \n",
      "Epoch: [1][14000/41910] Elapsed 169m 40s (remain 338m 12s) Loss: 0.1359(0.1451) Grad: 2.9714  LR: 0.00001336  \n",
      "Epoch: [1][14100/41910] Elapsed 170m 52s (remain 337m 0s) Loss: 0.0461(0.1446) Grad: 1.7408  LR: 0.00001346  \n",
      "Epoch: [1][14200/41910] Elapsed 172m 5s (remain 335m 47s) Loss: 0.1036(0.1441) Grad: 2.7604  LR: 0.00001355  \n",
      "Epoch: [1][14300/41910] Elapsed 173m 18s (remain 334m 34s) Loss: 0.1259(0.1436) Grad: 3.7892  LR: 0.00001365  \n",
      "Epoch: [1][14400/41910] Elapsed 174m 30s (remain 333m 21s) Loss: 0.0801(0.1431) Grad: 1.6985  LR: 0.00001374  \n"
     ]
    }
   ],
   "source": [
    "va_data.to_parquet('va_data_0_{}.parquet'.format(CFG.exp_name))\n",
    "val_result = train_loop(fold, model,tr_dataset, va_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#va_data.to_csv('val_0.csv', index=None)\n",
    "va_data = pd.read_parquet('va_data_0_exp1.parquet')  # random_negative_for_recall_exp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 105/105 [00:21<00:00,  4.99it/s]\n",
      "100%|██████████| 2407/2407 [07:59<00:00,  5.02it/s]\n",
      "100%|██████████| 6665/6665 [01:10<00:00, 94.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0368\n",
      "0.21595706789469252\n"
     ]
    }
   ],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "def infer_sbert(model, dataloader):\n",
    "    res = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids, attention_mask = [i.to(CFG.device) for i in batch]\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, attention_mask)\n",
    "            # Perform pooling\n",
    "            sentence_embeddings = mean_pooling(output, attention_mask)\n",
    "\n",
    "            # Normalize embeddings\n",
    "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "            res.append(sentence_embeddings.cpu().numpy())\n",
    "\n",
    "    return np.vstack(res)\n",
    "\n",
    "def recall(targets, preds): return len([x for x in targets if x in preds])/(len(targets)+ 1e-16)\n",
    "\n",
    "def f2_score(y_true, y_pred):\n",
    "\n",
    "    y_true = [set(i.split()) for i in y_true]\n",
    "    y_pred = [set(i.split()) for i in y_pred]\n",
    "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n",
    "    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n",
    "    precision = tp / (tp + fp)\n",
    "    recs = [recall(t,p) for t,p in list(zip(y_true, y_pred))]\n",
    "    f2 = tp / (tp + 0.2 * fp + 0.8 * fn)\n",
    "    return round(f2.mean(), 4), np.nanmean(recs)\n",
    "    \n",
    "def valid_fn(val_df, fold=0):\n",
    "    #CFG.model_path = 'sentence-transformers/all-MiniLM-L12-v2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "    val_topic_id = val_df['topic_id'].unique().tolist()\n",
    "    content_df = pd.read_csv('content.csv')\n",
    "    content_df = content_df.fillna('')\n",
    "    content_df['content_full_text'] = content_df['title'] + ' [SEP] ' + content_df['description'] + ' [SEP] ' + content_df['text']\n",
    "    content_df['content_full_text'] = content_df['content_full_text'].apply(lambda x:clean_text(x))\n",
    "    topic_df = pd.read_csv('topics.csv')\n",
    "    topic_df = topic_df[topic_df['id'].isin(val_topic_id)]\n",
    "    topic_df = topic_df.fillna('')\n",
    "    topic_df['topic_full_text'] = topic_df['title'] + ' [SEP] ' + topic_df['description']\n",
    "    topic_df['topic_full_text'] = topic_df['topic_full_text'].apply(lambda x:clean_text(x))\n",
    "    topic_dataset = TopicTestDataset(topic_df, tokenizer)\n",
    "    content_dataset = ContentTestDataset(content_df, tokenizer)\n",
    "    topic_loader = DataLoader(topic_dataset,\n",
    "                                  batch_size=CFG.batch_size * 2,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    content_loader = DataLoader(content_dataset,\n",
    "                                  batch_size=CFG.batch_size * 2,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    #model = AutoModel.from_pretrained(CFG.model_path)\n",
    "    model = Custom_Bert_Simple_Test()\n",
    "    model.load_state_dict(torch.load(CFG.OUTPUT_DIR + \"{}_{}_best{}.pth\".format(CFG.model_path.replace('/', '_'),CFG.exp_name,fold),strict=False))\n",
    "    model.to(CFG.device)\n",
    "    model.eval()\n",
    "    topic_result = infer(model, topic_loader)\n",
    "    content_result = infer(model, content_loader)\n",
    "    content_ids = [i for i in range(len(content_df))]\n",
    "    content_index = build_index(content_result, content_ids)\n",
    "    results = content_index.knn_query(topic_result, k = 100, num_threads = -1)\n",
    "    pred = []\n",
    "    content_uid = content_df['id']\n",
    "    for result in tqdm(results[0]):\n",
    "        top_same = ' '.join(content_uid[result].to_list())\n",
    "        pred.append(top_same)\n",
    "    corr_df_init = pd.read_csv('correlations.csv')\n",
    "    corr_df_init = corr_df_init[corr_df_init['topic_id'].isin(val_topic_id)]\n",
    "    val_corr_df = topic_df.merge(corr_df_init, how='left', left_on='id', right_on='topic_id')\n",
    "    val_corr_df['pred'] = pred\n",
    "    gts = val_corr_df['content_ids'].to_list()                                 \n",
    "    score, recall = f2_score(gts, pred)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    return score, recall, val_corr_df\n",
    "\n",
    "score, recall, val_corr_df = valid_fn(va_data)\n",
    "print(score)\n",
    "print( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>channel</th>\n",
       "      <th>category</th>\n",
       "      <th>level</th>\n",
       "      <th>language</th>\n",
       "      <th>parent</th>\n",
       "      <th>has_content</th>\n",
       "      <th>topic_full_text</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_001bcbb22694</td>\n",
       "      <td>Lección 1</td>\n",
       "      <td></td>\n",
       "      <td>6e90a7</td>\n",
       "      <td>aligned</td>\n",
       "      <td>6</td>\n",
       "      <td>es</td>\n",
       "      <td>t_5d10d6819e04</td>\n",
       "      <td>True</td>\n",
       "      <td>Lección 1 [SEP]</td>\n",
       "      <td>t_001bcbb22694</td>\n",
       "      <td>c_1d9dfc709413</td>\n",
       "      <td>c_87da6a40ebc8 c_f274e9c1688a c_31ff9848b09b c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_001c75b83927</td>\n",
       "      <td>Lección 2</td>\n",
       "      <td></td>\n",
       "      <td>6e90a7</td>\n",
       "      <td>aligned</td>\n",
       "      <td>6</td>\n",
       "      <td>es</td>\n",
       "      <td>t_b2ae11936b02</td>\n",
       "      <td>True</td>\n",
       "      <td>Lección 2 [SEP]</td>\n",
       "      <td>t_001c75b83927</td>\n",
       "      <td>c_60d8a4f8eff9</td>\n",
       "      <td>c_87da6a40ebc8 c_f274e9c1688a c_31ff9848b09b c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_0021d8020514</td>\n",
       "      <td>Lección 2</td>\n",
       "      <td></td>\n",
       "      <td>6e90a7</td>\n",
       "      <td>aligned</td>\n",
       "      <td>6</td>\n",
       "      <td>es</td>\n",
       "      <td>t_e26cb5145027</td>\n",
       "      <td>True</td>\n",
       "      <td>Lección 2 [SEP]</td>\n",
       "      <td>t_0021d8020514</td>\n",
       "      <td>c_e7e44cb2c32d</td>\n",
       "      <td>c_87da6a40ebc8 c_f274e9c1688a c_31ff9848b09b c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_002dfcaaf1d7</td>\n",
       "      <td>2.9: L'Hôpital's Rule</td>\n",
       "      <td></td>\n",
       "      <td>1fb613</td>\n",
       "      <td>supplemental</td>\n",
       "      <td>5</td>\n",
       "      <td>en</td>\n",
       "      <td>t_e19c46e71ee3</td>\n",
       "      <td>True</td>\n",
       "      <td>2.9: L'Hôpital's Rule [SEP]</td>\n",
       "      <td>t_002dfcaaf1d7</td>\n",
       "      <td>c_7c35d77064e5</td>\n",
       "      <td>c_5d40a2fae718 c_c62372581afa c_e12deaea53ef c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_003e944a4758</td>\n",
       "      <td>Lección 12</td>\n",
       "      <td></td>\n",
       "      <td>6e90a7</td>\n",
       "      <td>aligned</td>\n",
       "      <td>6</td>\n",
       "      <td>es</td>\n",
       "      <td>t_c059c108eb80</td>\n",
       "      <td>True</td>\n",
       "      <td>Lección 12 [SEP]</td>\n",
       "      <td>t_003e944a4758</td>\n",
       "      <td>c_8f6966ad85f6</td>\n",
       "      <td>c_87da6a40ebc8 c_f274e9c1688a c_43df58c3332a c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                  title description channel      category  \\\n",
       "0  t_001bcbb22694              Lección 1              6e90a7       aligned   \n",
       "1  t_001c75b83927              Lección 2              6e90a7       aligned   \n",
       "2  t_0021d8020514              Lección 2              6e90a7       aligned   \n",
       "3  t_002dfcaaf1d7  2.9: L'Hôpital's Rule              1fb613  supplemental   \n",
       "4  t_003e944a4758             Lección 12              6e90a7       aligned   \n",
       "\n",
       "   level language          parent  has_content               topic_full_text  \\\n",
       "0      6       es  t_5d10d6819e04         True              Lección 1 [SEP]    \n",
       "1      6       es  t_b2ae11936b02         True              Lección 2 [SEP]    \n",
       "2      6       es  t_e26cb5145027         True              Lección 2 [SEP]    \n",
       "3      5       en  t_e19c46e71ee3         True  2.9: L'Hôpital's Rule [SEP]    \n",
       "4      6       es  t_c059c108eb80         True             Lección 12 [SEP]    \n",
       "\n",
       "         topic_id     content_ids  \\\n",
       "0  t_001bcbb22694  c_1d9dfc709413   \n",
       "1  t_001c75b83927  c_60d8a4f8eff9   \n",
       "2  t_0021d8020514  c_e7e44cb2c32d   \n",
       "3  t_002dfcaaf1d7  c_7c35d77064e5   \n",
       "4  t_003e944a4758  c_8f6966ad85f6   \n",
       "\n",
       "                                                pred  \n",
       "0  c_87da6a40ebc8 c_f274e9c1688a c_31ff9848b09b c...  \n",
       "1  c_87da6a40ebc8 c_f274e9c1688a c_31ff9848b09b c...  \n",
       "2  c_87da6a40ebc8 c_f274e9c1688a c_31ff9848b09b c...  \n",
       "3  c_5d40a2fae718 c_c62372581afa c_e12deaea53ef c...  \n",
       "4  c_87da6a40ebc8 c_f274e9c1688a c_43df58c3332a c...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_corr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x1, x2):\n",
    "    return x1 in x2\n",
    "val_corr_df['if_recall'] = val_corr_df.apply(lambda x: f(x['content_ids'], x['pred']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.155138784696174"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(val_corr_df, 1, 0).sum() / len(val_corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>channel</th>\n",
       "      <th>category</th>\n",
       "      <th>level</th>\n",
       "      <th>language</th>\n",
       "      <th>parent</th>\n",
       "      <th>has_content</th>\n",
       "      <th>topic_full_text</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "      <th>pred</th>\n",
       "      <th>if_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_001bcbb22694</td>\n",
       "      <td>Lección 1</td>\n",
       "      <td></td>\n",
       "      <td>6e90a7</td>\n",
       "      <td>aligned</td>\n",
       "      <td>6</td>\n",
       "      <td>es</td>\n",
       "      <td>t_5d10d6819e04</td>\n",
       "      <td>True</td>\n",
       "      <td>Lección 1 [SEP]</td>\n",
       "      <td>t_001bcbb22694</td>\n",
       "      <td>c_1d9dfc709413</td>\n",
       "      <td>c_87da6a40ebc8 c_f274e9c1688a c_31ff9848b09b c...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_001c75b83927</td>\n",
       "      <td>Lección 2</td>\n",
       "      <td></td>\n",
       "      <td>6e90a7</td>\n",
       "      <td>aligned</td>\n",
       "      <td>6</td>\n",
       "      <td>es</td>\n",
       "      <td>t_b2ae11936b02</td>\n",
       "      <td>True</td>\n",
       "      <td>Lección 2 [SEP]</td>\n",
       "      <td>t_001c75b83927</td>\n",
       "      <td>c_60d8a4f8eff9</td>\n",
       "      <td>c_87da6a40ebc8 c_f274e9c1688a c_31ff9848b09b c...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_0021d8020514</td>\n",
       "      <td>Lección 2</td>\n",
       "      <td></td>\n",
       "      <td>6e90a7</td>\n",
       "      <td>aligned</td>\n",
       "      <td>6</td>\n",
       "      <td>es</td>\n",
       "      <td>t_e26cb5145027</td>\n",
       "      <td>True</td>\n",
       "      <td>Lección 2 [SEP]</td>\n",
       "      <td>t_0021d8020514</td>\n",
       "      <td>c_e7e44cb2c32d</td>\n",
       "      <td>c_87da6a40ebc8 c_f274e9c1688a c_31ff9848b09b c...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_002dfcaaf1d7</td>\n",
       "      <td>2.9: L'Hôpital's Rule</td>\n",
       "      <td></td>\n",
       "      <td>1fb613</td>\n",
       "      <td>supplemental</td>\n",
       "      <td>5</td>\n",
       "      <td>en</td>\n",
       "      <td>t_e19c46e71ee3</td>\n",
       "      <td>True</td>\n",
       "      <td>2.9: L'Hôpital's Rule [SEP]</td>\n",
       "      <td>t_002dfcaaf1d7</td>\n",
       "      <td>c_7c35d77064e5</td>\n",
       "      <td>c_5d40a2fae718 c_c62372581afa c_e12deaea53ef c...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_003e944a4758</td>\n",
       "      <td>Lección 12</td>\n",
       "      <td></td>\n",
       "      <td>6e90a7</td>\n",
       "      <td>aligned</td>\n",
       "      <td>6</td>\n",
       "      <td>es</td>\n",
       "      <td>t_c059c108eb80</td>\n",
       "      <td>True</td>\n",
       "      <td>Lección 12 [SEP]</td>\n",
       "      <td>t_003e944a4758</td>\n",
       "      <td>c_8f6966ad85f6</td>\n",
       "      <td>c_87da6a40ebc8 c_f274e9c1688a c_43df58c3332a c...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6660</th>\n",
       "      <td>t_ffd57d147a69</td>\n",
       "      <td>Lección 20</td>\n",
       "      <td></td>\n",
       "      <td>6e90a7</td>\n",
       "      <td>aligned</td>\n",
       "      <td>6</td>\n",
       "      <td>es</td>\n",
       "      <td>t_83ed18a38507</td>\n",
       "      <td>True</td>\n",
       "      <td>Lección 20 [SEP]</td>\n",
       "      <td>t_ffd57d147a69</td>\n",
       "      <td>c_02c73701948c</td>\n",
       "      <td>c_87da6a40ebc8 c_31ff9848b09b c_7048090da02f c...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6661</th>\n",
       "      <td>t_ffdc013937fc</td>\n",
       "      <td>Book: Introduction to Algebraic Structures (De...</td>\n",
       "      <td></td>\n",
       "      <td>1fb613</td>\n",
       "      <td>supplemental</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>t_6888f65a0882</td>\n",
       "      <td>True</td>\n",
       "      <td>Book: Introduction to Algebraic Structures (De...</td>\n",
       "      <td>t_ffdc013937fc</td>\n",
       "      <td>c_c27c5e711e25</td>\n",
       "      <td>c_cd80b4931223 c_e7ce72a553bd c_b16f03694474 c...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6662</th>\n",
       "      <td>t_fff05585df72</td>\n",
       "      <td>11: Systems of Equations and Inequalities</td>\n",
       "      <td></td>\n",
       "      <td>1fb613</td>\n",
       "      <td>supplemental</td>\n",
       "      <td>4</td>\n",
       "      <td>en</td>\n",
       "      <td>t_5ab3d2eac617</td>\n",
       "      <td>True</td>\n",
       "      <td>11: Systems of Equations and Inequalities [SEP]</td>\n",
       "      <td>t_fff05585df72</td>\n",
       "      <td>c_6f255c97f381 c_743e6319d5ae c_88bc7ee86c8b c...</td>\n",
       "      <td>c_933ec74fe303 c_88bc7ee86c8b c_ad9da9f1a277 c...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6663</th>\n",
       "      <td>t_fff7782561f4</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>In certain situations, comparison by division ...</td>\n",
       "      <td>d5fb04</td>\n",
       "      <td>supplemental</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>t_2a4dc28b0431</td>\n",
       "      <td>True</td>\n",
       "      <td>Introduction [SEP] In certain situations, comp...</td>\n",
       "      <td>t_fff7782561f4</td>\n",
       "      <td>c_bca8280a9ad1</td>\n",
       "      <td>c_58e4b13049f6 c_8760f3b2c12f c_69b61f90d63e c...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6664</th>\n",
       "      <td>t_fffe14f1be1e</td>\n",
       "      <td>Lección 7</td>\n",
       "      <td></td>\n",
       "      <td>6e90a7</td>\n",
       "      <td>aligned</td>\n",
       "      <td>6</td>\n",
       "      <td>es</td>\n",
       "      <td>t_d448c707984d</td>\n",
       "      <td>True</td>\n",
       "      <td>Lección 7 [SEP]</td>\n",
       "      <td>t_fffe14f1be1e</td>\n",
       "      <td>c_cece166bad6a</td>\n",
       "      <td>c_87da6a40ebc8 c_7048090da02f c_e8e2319c92a7 c...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6665 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                              title  \\\n",
       "0     t_001bcbb22694                                          Lección 1   \n",
       "1     t_001c75b83927                                          Lección 2   \n",
       "2     t_0021d8020514                                          Lección 2   \n",
       "3     t_002dfcaaf1d7                              2.9: L'Hôpital's Rule   \n",
       "4     t_003e944a4758                                         Lección 12   \n",
       "...              ...                                                ...   \n",
       "6660  t_ffd57d147a69                                         Lección 20   \n",
       "6661  t_ffdc013937fc  Book: Introduction to Algebraic Structures (De...   \n",
       "6662  t_fff05585df72          11: Systems of Equations and Inequalities   \n",
       "6663  t_fff7782561f4                                       Introduction   \n",
       "6664  t_fffe14f1be1e                                          Lección 7   \n",
       "\n",
       "                                            description channel      category  \\\n",
       "0                                                        6e90a7       aligned   \n",
       "1                                                        6e90a7       aligned   \n",
       "2                                                        6e90a7       aligned   \n",
       "3                                                        1fb613  supplemental   \n",
       "4                                                        6e90a7       aligned   \n",
       "...                                                 ...     ...           ...   \n",
       "6660                                                     6e90a7       aligned   \n",
       "6661                                                     1fb613  supplemental   \n",
       "6662                                                     1fb613  supplemental   \n",
       "6663  In certain situations, comparison by division ...  d5fb04  supplemental   \n",
       "6664                                                     6e90a7       aligned   \n",
       "\n",
       "      level language          parent  has_content  \\\n",
       "0         6       es  t_5d10d6819e04         True   \n",
       "1         6       es  t_b2ae11936b02         True   \n",
       "2         6       es  t_e26cb5145027         True   \n",
       "3         5       en  t_e19c46e71ee3         True   \n",
       "4         6       es  t_c059c108eb80         True   \n",
       "...     ...      ...             ...          ...   \n",
       "6660      6       es  t_83ed18a38507         True   \n",
       "6661      3       en  t_6888f65a0882         True   \n",
       "6662      4       en  t_5ab3d2eac617         True   \n",
       "6663      3       en  t_2a4dc28b0431         True   \n",
       "6664      6       es  t_d448c707984d         True   \n",
       "\n",
       "                                        topic_full_text        topic_id  \\\n",
       "0                                      Lección 1 [SEP]   t_001bcbb22694   \n",
       "1                                      Lección 2 [SEP]   t_001c75b83927   \n",
       "2                                      Lección 2 [SEP]   t_0021d8020514   \n",
       "3                          2.9: L'Hôpital's Rule [SEP]   t_002dfcaaf1d7   \n",
       "4                                     Lección 12 [SEP]   t_003e944a4758   \n",
       "...                                                 ...             ...   \n",
       "6660                                  Lección 20 [SEP]   t_ffd57d147a69   \n",
       "6661  Book: Introduction to Algebraic Structures (De...  t_ffdc013937fc   \n",
       "6662   11: Systems of Equations and Inequalities [SEP]   t_fff05585df72   \n",
       "6663  Introduction [SEP] In certain situations, comp...  t_fff7782561f4   \n",
       "6664                                   Lección 7 [SEP]   t_fffe14f1be1e   \n",
       "\n",
       "                                            content_ids  \\\n",
       "0                                        c_1d9dfc709413   \n",
       "1                                        c_60d8a4f8eff9   \n",
       "2                                        c_e7e44cb2c32d   \n",
       "3                                        c_7c35d77064e5   \n",
       "4                                        c_8f6966ad85f6   \n",
       "...                                                 ...   \n",
       "6660                                     c_02c73701948c   \n",
       "6661                                     c_c27c5e711e25   \n",
       "6662  c_6f255c97f381 c_743e6319d5ae c_88bc7ee86c8b c...   \n",
       "6663                                     c_bca8280a9ad1   \n",
       "6664                                     c_cece166bad6a   \n",
       "\n",
       "                                                   pred  if_recall  \n",
       "0     c_87da6a40ebc8 c_f274e9c1688a c_31ff9848b09b c...       True  \n",
       "1     c_87da6a40ebc8 c_f274e9c1688a c_31ff9848b09b c...      False  \n",
       "2     c_87da6a40ebc8 c_f274e9c1688a c_31ff9848b09b c...      False  \n",
       "3     c_5d40a2fae718 c_c62372581afa c_e12deaea53ef c...      False  \n",
       "4     c_87da6a40ebc8 c_f274e9c1688a c_43df58c3332a c...      False  \n",
       "...                                                 ...        ...  \n",
       "6660  c_87da6a40ebc8 c_31ff9848b09b c_7048090da02f c...      False  \n",
       "6661  c_cd80b4931223 c_e7ce72a553bd c_b16f03694474 c...       True  \n",
       "6662  c_933ec74fe303 c_88bc7ee86c8b c_ad9da9f1a277 c...      False  \n",
       "6663  c_58e4b13049f6 c_8760f3b2c12f c_69b61f90d63e c...       True  \n",
       "6664  c_87da6a40ebc8 c_7048090da02f c_e8e2319c92a7 c...      False  \n",
       "\n",
       "[6665 rows x 14 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
