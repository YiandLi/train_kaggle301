{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyfn8McqUEBQ"
   },
   "source": [
    "## CV Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install multiprocesspandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XfVCwUiETLWX"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rank_bm25'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmultiprocesspandas\u001b[39;00m \u001b[39mimport\u001b[39;00m applyparallel\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrank_bm25\u001b[39;00m \u001b[39mimport\u001b[39;00m BM25Okapi, BM25L\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rank_bm25'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from multiprocesspandas import applyparallel\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi, BM25L\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KpzDtQBUUCd1"
   },
   "outputs": [],
   "source": [
    "input_path = r'../input_dir/'\n",
    "topic_df = pd.read_csv(input_path + r'topics.csv')\n",
    "content_df = pd.read_csv(input_path + r'content.csv')\n",
    "corr_df = pd.read_csv(input_path + r'correlations.csv')\n",
    "topic_df = topic_df.rename(columns={'id': 'topic_id'}).merge(corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb145ef92f94ceda16e2e0975a27ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab49135cf024905aff948afaa8cc630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddcde701b204328b9ccae18fa1dbc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e9L5F7uqWhpl"
   },
   "outputs": [],
   "source": [
    "corr_df['content_ids'] = corr_df['content_ids'].apply(lambda x:x.split())\n",
    "corr_df = corr_df.explode('content_ids').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "r94vw5-gXHoX"
   },
   "outputs": [],
   "source": [
    "topic_df = topic_df.fillna('')\n",
    "topic_df['topic_full_text'] =  topic_df['title'] + ' [SEP] ' + topic_df['description']\n",
    "topic_df = topic_df[['topic_id', 'topic_full_text', 'language', 'title']]\n",
    "df = corr_df.merge(topic_df, left_on='topic_id', right_on='topic_id', how='left')\n",
    "df = df[['topic_id','content_ids','topic_full_text','language', 'title']]\n",
    "df = df.rename(columns={'language':'topic_language', 'title':'topic_title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DA20jcVaXkkP"
   },
   "outputs": [],
   "source": [
    "content_df = content_df.fillna('')\n",
    "content_df['content_full_text'] =  content_df['title'] + ' [SEP] ' + content_df['description'] + ' [SEP] ' + content_df['text']\n",
    "content_df = content_df[['id', 'content_full_text', 'language', 'title']]\n",
    "df = df.merge(content_df, left_on='content_ids', right_on='id', how='left')\n",
    "df = df.rename(columns={'language':'content_language', 'title': 'content_title'})\n",
    "df['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "      <th>topic_full_text</th>\n",
       "      <th>topic_language</th>\n",
       "      <th>topic_title</th>\n",
       "      <th>id</th>\n",
       "      <th>content_full_text</th>\n",
       "      <th>content_language</th>\n",
       "      <th>content_title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_1108dd0c7a5d</td>\n",
       "      <td>Откриването на резисторите [SEP] Изследване на...</td>\n",
       "      <td>bg</td>\n",
       "      <td>Откриването на резисторите</td>\n",
       "      <td>c_1108dd0c7a5d</td>\n",
       "      <td>Молив като резистор [SEP] Моливът причинява пр...</td>\n",
       "      <td>bg</td>\n",
       "      <td>Молив като резистор</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_376c5a8eb028</td>\n",
       "      <td>Откриването на резисторите [SEP] Изследване на...</td>\n",
       "      <td>bg</td>\n",
       "      <td>Откриването на резисторите</td>\n",
       "      <td>c_376c5a8eb028</td>\n",
       "      <td>Да чуем променливото съпротивление [SEP] Тук ч...</td>\n",
       "      <td>bg</td>\n",
       "      <td>Да чуем променливото съпротивление</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_5bc0e1e2cba0</td>\n",
       "      <td>Откриването на резисторите [SEP] Изследване на...</td>\n",
       "      <td>bg</td>\n",
       "      <td>Откриването на резисторите</td>\n",
       "      <td>c_5bc0e1e2cba0</td>\n",
       "      <td>Променлив резистор (реостат) с графит от молив...</td>\n",
       "      <td>bg</td>\n",
       "      <td>Променлив резистор (реостат) с графит от молив</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_76231f9d0b5e</td>\n",
       "      <td>Откриването на резисторите [SEP] Изследване на...</td>\n",
       "      <td>bg</td>\n",
       "      <td>Откриването на резисторите</td>\n",
       "      <td>c_76231f9d0b5e</td>\n",
       "      <td>Последователно свързване на галваничен елемент...</td>\n",
       "      <td>bg</td>\n",
       "      <td>Последователно свързване на галваничен елемент...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_00068291e9a4</td>\n",
       "      <td>c_639ea2ef9c95</td>\n",
       "      <td>Entradas e saídas de uma função [SEP] Entenda ...</td>\n",
       "      <td>pt</td>\n",
       "      <td>Entradas e saídas de uma função</td>\n",
       "      <td>c_639ea2ef9c95</td>\n",
       "      <td>Dados e resultados de funções: gráficos [SEP] ...</td>\n",
       "      <td>pt</td>\n",
       "      <td>Dados e resultados de funções: gráficos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_id     content_ids  \\\n",
       "0  t_00004da3a1b2  c_1108dd0c7a5d   \n",
       "1  t_00004da3a1b2  c_376c5a8eb028   \n",
       "2  t_00004da3a1b2  c_5bc0e1e2cba0   \n",
       "3  t_00004da3a1b2  c_76231f9d0b5e   \n",
       "4  t_00068291e9a4  c_639ea2ef9c95   \n",
       "\n",
       "                                     topic_full_text topic_language  \\\n",
       "0  Откриването на резисторите [SEP] Изследване на...             bg   \n",
       "1  Откриването на резисторите [SEP] Изследване на...             bg   \n",
       "2  Откриването на резисторите [SEP] Изследване на...             bg   \n",
       "3  Откриването на резисторите [SEP] Изследване на...             bg   \n",
       "4  Entradas e saídas de uma função [SEP] Entenda ...             pt   \n",
       "\n",
       "                       topic_title              id  \\\n",
       "0       Откриването на резисторите  c_1108dd0c7a5d   \n",
       "1       Откриването на резисторите  c_376c5a8eb028   \n",
       "2       Откриването на резисторите  c_5bc0e1e2cba0   \n",
       "3       Откриването на резисторите  c_76231f9d0b5e   \n",
       "4  Entradas e saídas de uma função  c_639ea2ef9c95   \n",
       "\n",
       "                                   content_full_text content_language  \\\n",
       "0  Молив като резистор [SEP] Моливът причинява пр...               bg   \n",
       "1  Да чуем променливото съпротивление [SEP] Тук ч...               bg   \n",
       "2  Променлив резистор (реостат) с графит от молив...               bg   \n",
       "3  Последователно свързване на галваничен елемент...               bg   \n",
       "4  Dados e resultados de funções: gráficos [SEP] ...               pt   \n",
       "\n",
       "                                       content_title  label  \n",
       "0                                Молив като резистор      1  \n",
       "1                 Да чуем променливото съпротивление      1  \n",
       "2     Променлив резистор (реостат) с графит от молив      1  \n",
       "3  Последователно свързване на галваничен елемент...      1  \n",
       "4            Dados e resultados de funções: gráficos      1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bm2.5 or tfidf 采样 较难"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bm25_negative(sentence, candidates_sentence, thres=2):\n",
    "    tokenized_corpus = [tokenizer.tokenize(i) for i in candidates_sentence]\n",
    "    bm25 = BM25L(tokenized_corpus)\n",
    "    tokenized_query = tokenizer.tokenize(sentence)\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    index = np.argpartition(doc_scores, -2)[-2:]\n",
    "    return np.array(candidates_sentence)[index].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/61517 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1370 > 512). Running this sequence through the model will result in indexing errors\n",
      "  1%|          | 414/61517 [09:08<22:28:12,  1.32s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m/root/kaggle/code/LECR_negative_smaple_bm25.ipynb 单元格 11\u001b[0m in \u001b[0;36mget_bm25_negative\u001b[0;34m(sentence, candidates_sentence, thres)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bi-2.gpushare.com/root/kaggle/code/LECR_negative_smaple_bm25.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_bm25_negative\u001b[39m(sentence, candidates_sentence, thres\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bi-2.gpushare.com/root/kaggle/code/LECR_negative_smaple_bm25.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     tokenized_corpus \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39mtokenize(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m candidates_sentence]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bi-2.gpushare.com/root/kaggle/code/LECR_negative_smaple_bm25.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     bm25 \u001b[39m=\u001b[39m BM25L(tokenized_corpus)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bi-2.gpushare.com/root/kaggle/code/LECR_negative_smaple_bm25.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     tokenized_query \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtokenize(sentence)\n",
      "\u001b[1;32m/root/kaggle/code/LECR_negative_smaple_bm25.ipynb 单元格 11\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bi-2.gpushare.com/root/kaggle/code/LECR_negative_smaple_bm25.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_bm25_negative\u001b[39m(sentence, candidates_sentence, thres\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bi-2.gpushare.com/root/kaggle/code/LECR_negative_smaple_bm25.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     tokenized_corpus \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39;49mtokenize(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m candidates_sentence]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bi-2.gpushare.com/root/kaggle/code/LECR_negative_smaple_bm25.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     bm25 \u001b[39m=\u001b[39m BM25L(tokenized_corpus)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bi-2.gpushare.com/root/kaggle/code/LECR_negative_smaple_bm25.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     tokenized_query \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtokenize(sentence)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_fast.py:320\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.tokenize\u001b[0;34m(self, text, pair, add_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, pair: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, add_special_tokens: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 320\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mpair, add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mtokens()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2702\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2693\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2694\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2695\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2699\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2700\u001b[0m )\n\u001b[0;32m-> 2702\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2703\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2704\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2705\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2706\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2707\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2708\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2709\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2710\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2711\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2712\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2713\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2714\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2715\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2716\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2717\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2718\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2719\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2720\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2721\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_fast.py:502\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    480\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    481\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    499\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    501\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[0;32m--> 502\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m    503\u001b[0m         batched_input,\n\u001b[1;32m    504\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    505\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    506\u001b[0m         padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    507\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    508\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    509\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    510\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    511\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    512\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    513\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    514\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    515\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    516\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m    517\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    518\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    519\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[1;32m    522\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    523\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    524\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_fast.py:429\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    422\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[1;32m    423\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    426\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    427\u001b[0m )\n\u001b[0;32m--> 429\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[1;32m    430\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    431\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    432\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    433\u001b[0m )\n\u001b[1;32m    435\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    441\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[1;32m    442\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[1;32m    443\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[1;32m    453\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#test_df = df[df['topic_id']=='t_000d1fb3f2f5']\n",
    "neg_df = []\n",
    "for topic_id in tqdm(df['topic_id'].unique()):\n",
    "    sub_df = df[df['topic_id'] == topic_id]\n",
    "    topic_language = sub_df['topic_language'].unique()[0]\n",
    "    topic_full_text = sub_df['topic_full_text'].unique()[0]\n",
    "    ## bm25\n",
    "    bm25 = []\n",
    "    \n",
    "    querys = sub_df['content_full_text'].to_list()\n",
    "    for i in querys:\n",
    "        candidates = df[df['content_language'] == topic_language].sample(n=30)\n",
    "        candidates_sentences = candidates['content_full_text'].to_list()\n",
    "        bm25_negative = pd.DataFrame({'topic_full_text':[topic_full_text],\n",
    "                                      'content_full_text':['']\n",
    "                                     })\n",
    "        results = get_bm25_negative(i, candidates_sentences)\n",
    "        bm25_negative['content_full_text'] = '\\u00001'.join(results)\n",
    "        bm25_negative['topic_id'] = topic_id\n",
    "        bm25_negative['label'] = 0\n",
    "        bm25_negative['content_full_text'] = bm25_negative['content_full_text'].apply(lambda x:x.split('\\u00001'))\n",
    "        bm25_negative = bm25_negative.explode('content_full_text').reset_index(drop=True)\n",
    "        bm25.append(bm25_negative)\n",
    "    \n",
    "    bm25 = pd.concat(bm25)\n",
    "    bm25 = bm25[-(bm25['content_full_text'].isin(sub_df['content_full_text'].to_list()))]\n",
    "\n",
    "    neg_df.append(bm25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r'/root/kaggle/output_dir/'\n",
    "neg_df = pd.concat(neg_df)\n",
    "neg_df = neg_df.drop_duplicates()\n",
    "neg_df.to_csv(output_path + r'bm25_negative.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102629, 9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
